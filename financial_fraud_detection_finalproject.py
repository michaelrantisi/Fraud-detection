# -*- coding: utf-8 -*-
"""financial fraud detection_FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12vsAi1NPYqZ4NPAkrh0VDWq8eqXaubus
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras import models, layers
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV

train_path = '/content/train_v2.csv'
data_loan_train = pd.read_csv(train_path)
test_path='/content/test_v2.csv'
data_loan_test=pd.read_csv(test_path)

data_loan_train.head()

data_loan_test.head()

data_loan_train.isnull().sum()

data_loan_test.isnull().sum()

data_loan_tr=data_loan_train.dropna()
data_loan_te=data_loan_test.dropna()

data_loan_tr.isnull().sum()

data_loan_te.isnull().sum()

sns.histplot(data_loan_tr['loss'])
plt.title('Histogram Showing Loss Distribution')

number = data_loan_tr.f777.value_counts()
legal = number[0]
fraud = number[1]
legal_perc = (legal/(legal+fraud))*100
fraud_perc = (fraud/(legal+fraud))*100
print("There were {} legal transactions ({:.2f}%) and {} fraud transactions ({:.2f}%).".format(legal, legal_perc, fraud, fraud_perc))

sns.barplot(x=number.index, y=number)
plt.title("Number of Legal vs. Fraud Transactions")
plt.ylabel("Number of transactions")
plt.xlabel("Class Variable (0:Legal, 1:Fraud)")
plt.show()

number_loan = data_loan_tr.f776.value_counts()
sns.barplot(x=number_loan.index, y=number_loan)
plt.title("Loans Taken or Not")
plt.ylabel("Number of transactions")
plt.xlabel("Class Variable (0:Taken, 1:Not Taken)")
plt.show()

plt.scatter(data_loan_tr['id'], data_loan_tr['loss'])
plt.xlabel('ID')
plt.ylabel('Loss')
plt.title('Plot Showing Distribution of Loss with ID')
plt.show()

print("Train Number: " + str(data_loan_tr.shape) + " \nTest Number: " + str(data_loan_te.shape))
data_loan_tr.reset_index(drop=True, inplace=True)
data_loan_te.reset_index(drop=True, inplace=True)

fraud_in_train = data_loan_tr.f777.value_counts()[1]
print("There are " + str(fraud_in_train) + " fraud transactions in train data.")

df = data_loan_train.copy()
df.replace("NA",np.NaN)
df.fillna(df.mean(), inplace = True)
df = df.dropna()
df["loss"] = df["loss"].apply(lambda x: 0 if x== 0 else 1)

data_loan_tr.info()

data_loan_te.describe()

data_loan_tr.describe()

fd=data_loan_tr['f777'].value_counts()
fd

df = pd.concat([data_loan_te, data_loan_tr])

df.head()

data=df.dropna()
data.head()

def prep_data(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    X = df.iloc[:, 2:30].values
    y = df['f777'].values
    return X, y

X, y = prep_data(data)

X, y = prep_data(data)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


pca = PCA(n_components=28)
principalComponents_train = pca.fit_transform(X_train_scaled)
principalComponents_test = pca.transform(X_test_scaled)

cumsum = np.cumsum(pca.explained_variance_ratio_)

plt.plot(cumsum)
plt.xlabel("Principal Components")
plt.ylabel("Cumulative Variance Explained")

d = np.argmax(cumsum >= 0.95) + 1

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)


print(d)

pca = PCA(n_components=28)
principalComponents_train = pca.fit_transform(X_train)
principalComponents_test = pca.fit_transform(X_test)

model = LinearRegression()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model.fit(X_train, y_train)
y_predicted = model.predict(X_test)
r2_score(y_test, y_predicted)

model_log = LogisticRegression(solver='liblinear')
model_log.fit(X_train, y_train)
predicted = model_log.predict(X_test)
r2_score(y_test, predicted)

model_rfc=RandomForestClassifier(max_depth=2, random_state=0)
model_rfc.fit(X_train, y_train)
predicted = model_rfc.predict(X_test)
r2_score(y_test, predicted)

model_rfc.fit(principalComponents_train,y_train)
round(model_rfc.score(principalComponents_train,y_train))

model_gnb = GaussianNB()
model_gnb.fit(X_train, y_train)
predicted_gnb = model_gnb.predict(X_test)
r2_gnb = r2_score(y_test, predicted_gnb)
print(f"Gaussian Naive Bayes R^2: {r2_gnb}")

model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model_xgb.fit(X_train, y_train)
predicted_xgb = model_xgb.predict(X_test)
r2_xgb = r2_score(y_test, predicted_xgb)
print(f"XGBoost R^2: {r2_xgb}")

"""##Logistical Regression



"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=28)
principalComponents_train = pca.fit_transform(X_train)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_train_pred = logreg.predict(X_train)
train_accuracy = logreg.score(X_train, y_train)
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
class_report = classification_report(y_train, y_train_pred, digits=3)
print("Classification Report on Training Set:\n", class_report)

y_test_pred = logreg.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
con_matrix = confusion_matrix(y_test, y_test_pred)
print("Confusion Matrix:\n", con_matrix)
class_report = classification_report(y_test, y_test_pred, digits=3)
print("Classification Report:\n", class_report)

"""##Linear Regression"""

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)
y_pred = linear_reg.predict(X_train)

mse = mean_squared_error(y_train, y_pred)
mae = mean_absolute_error(y_train, y_pred)
rmse = mean_squared_error(y_train, y_pred, squared=False)
r2 = r2_score(y_train, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R²): {r2:.2f}")

y_test_pred = linear_reg.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)
r2_test = r2_score(y_test, y_test_pred)

print(f"Test Mean Squared Error (MSE): {mse_test:.2f}")
print(f"Test Mean Absolute Error (MAE): {mae_test:.2f}")
print(f"Test Root Mean Squared Error (RMSE): {rmse_test:.2f}")
print(f"Test R-squared (R²): {r2_test:.2f}")

"""##RandomForest"""

random_forest=RandomForestClassifier(n_estimators=100,random_state=0)
random_forest.fit(X_train,y_train)
y_predictions=random_forest.predict(X_train)
random_forest.score(X_train,y_train)
acc_random_forest=round(random_forest.score(X_train,y_train)*100,2)
print(acc_random_forest)
con=confusion_matrix(y_train,y_predictions)
print(con)
print(classification_report(y_train,y_predictions,digits=3))

y_test_pred = random_forest.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
con_matrix = confusion_matrix(y_test, y_test_pred)
print("Confusion Matrix:\n", con_matrix)
class_report = classification_report(y_test, y_test_pred, digits=3)
print("Classification Report:\n", class_report)

"""##Gaussian"""

gaussian_nb = GaussianNB()
gaussian_nb.fit(X_train, y_train)
y_train_pred = gaussian_nb.predict(X_train)
train_accuracy_nb = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {train_accuracy_nb * 100:.2f}%")
con_matrix_train = confusion_matrix(y_train, y_train_pred)
print("Confusion Matrix on Training Set:\n", con_matrix_train)
class_report_train = classification_report(y_train, y_train_pred, digits=3)
print("Classification Report on Training Set:\n", class_report_train)

y_test_pred = gaussian_nb.predict(X_test)
test_accuracy_nb = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy_nb * 100:.2f}%")
con_matrix_test = confusion_matrix(y_test, y_test_pred)
print("Confusion Matrix on Test Set:\n", con_matrix_test)
class_report_test = classification_report(y_test, y_test_pred, digits=3)
print("Classification Report on Test Set:\n", class_report_test)

"""##XGBOOST"""

xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=0)
xgb_clf.fit(X_train, y_train)
y_train_pred_xgb = xgb_clf.predict(X_train)
train_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)
print(f"Training Accuracy: {train_accuracy_xgb * 100:.2f}%")
con_matrix_train_xgb = confusion_matrix(y_train, y_train_pred_xgb)
print("Confusion Matrix on Training Set:\n", con_matrix_train_xgb)
class_report_train_xgb = classification_report(y_train, y_train_pred_xgb, digits=3)
print("Classification Report on Training Set:\n", class_report_train_xgb)

y_test_pred_xgb = xgb_clf.predict(X_test)
test_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)
print(f"Test Accuracy: {test_accuracy_xgb * 100:.2f}%")
con_matrix_test_xgb = confusion_matrix(y_test, y_test_pred_xgb)
print("Confusion Matrix on Test Set:\n", con_matrix_test_xgb)
class_report_test_xgb = classification_report(y_test, y_test_pred_xgb, digits=3)
print("Classification Report on Test Set:\n", class_report_test_xgb)

models = {"Logistic Regression": LogisticRegression(max_iter = 200),
        "Random Forest": RandomForestClassifier(),
        "Gaussian NB":GaussianNB(),
        'XGBoost': XGBClassifier()
}

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))
axes = axes.flatten()

for cnt, (name, model) in enumerate(models.items()):
    model.fit(principalComponents_train, y_train)
    model_pred = model.predict(principalComponents_test)

    conf_m = confusion_matrix(y_test, model_pred)
    display = ConfusionMatrixDisplay(confusion_matrix=conf_m, display_labels=['Non defaulters', 'Defaulters'])
    display.plot(ax=axes[cnt], colorbar=True, cmap="plasma")
    axes[cnt].set_title(f"{name} Confusion Matrix")

plt.tight_layout()
plt.show()

param_grid_rfc = {
    'n_estimators': [10, 50, 100],
    'max_depth': [None, 10, 20, 30],

}

grid_search_rfc = GridSearchCV(RandomForestClassifier(), param_grid_rfc, cv=5)

grid_search_rfc.fit(principalComponents_train, y_train)
best_model_rfc = grid_search_rfc.best_estimator_
rfc_pred = best_model_rfc.predict(principalComponents_test)
rfc_accuracy = accuracy_score(y_test, rfc_pred)
print(f"New accuracy for Random Forest after grid search: {rfc_accuracy}")

param_grid_log_reg = {
    'C': np.logspace(-4, 4, 20),
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

grid_search_log_reg = GridSearchCV(LogisticRegression(max_iter=200), param_grid_log_reg, cv=5)
grid_search_log_reg.fit(principalComponents_train, y_train)


best_model_log_reg = grid_search_log_reg.best_estimator_
log_reg_pred = best_model_log_reg.predict(principalComponents_test)
log_reg_accuracy = accuracy_score(y_test, log_reg_pred)
print(f"New accuracy for Logistic Regression after grid search: {log_reg_accuracy}")

param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.5],

}

grid_search_xgb = GridSearchCV(XGBClassifier(), param_grid_xgb, cv=5, verbose=1, n_jobs=-1)

grid_search_xgb.fit(principalComponents_train, y_train)
best_model_xgb = grid_search_xgb.best_estimator_

best_model_xgb = grid_search_xgb.best_estimator_
xgb_pred = best_model_xgb.predict(principalComponents_test)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
print(f"New accuracy for XGBoost after grid search: {xgb_accuracy}")

param_grid_nb = {
    'var_smoothing': np.logspace(0, -9, num=100)
}

grid_search_nb = GridSearchCV(GaussianNB(), param_grid_nb, cv=5)

grid_search_nb.fit(principalComponents_train, y_train)
best_model_nb = grid_search_nb.best_estimator_
best_model_nb = grid_search_nb.best_estimator_
nb_pred = best_model_nb.predict(principalComponents_test)
nb_accuracy = accuracy_score(y_test, nb_pred)
print(f"New accuracy for Gaussian NB after grid search: {nb_accuracy}")